from Bishop import *

import csv
import pandas as pd

# Initialize which participant we're processing (we're parallelizing outside 
# of this script to handle multiple participants).
PARTICIPANT = sys.argv[1]

# Initialize the number of reward functions we want to sample.
NUM_SAMPLES = 1

# Initialize a list of dictionaries that links maps to their doors and 
# observations.
STIMULI = [
	{"map": "DX_DX_0", "doors": "16 5-56 55", "observation": "6 4"},
	{"map": "DX_NX_0", "doors": "16 5-104 115", "observation": "5 4"},
	{"map": "DX_PX_0", "doors": "15 4-86 87", "observation": "4 6"},
	{"map": "DX_UN_0", "doors": "104 115-34 33", "observation": "5 5"},
	{"map": "ND_DX_0", "doors": "64 65", "observation": "4 4"},
	{"map": "ND_DX_1", "doors": "42 43", "observation": "2 6"},
	{"map": "ND_NX_0", "doors": "64 65", "observation": "5 4"},
	{"map": "ND_NX_1", "doors": "42 43", "observation": "3 7"},
	{"map": "ND_PX_0", "doors": "42 43", "observation": "3 5"},
	{"map": "ND_PX_1", "doors": "53 54", "observation": "2 4"},
	{"map": "ND_UN_0", "doors": "86 87", "observation": "7 6"},
	{"map": "NX_DX_0", "doors": "64 65-106 117-56 55", "observation": "6 5"},
	{"map": "NX_NX_0", "doors": "14 3-53 54-105 116", "observation": "5 5"},
	{"map": "NX_PX_0", "doors": "75 76-105 116-34 33", "observation": "5 3"},
	{"map": "NX_UN_0", "doors": "64 65-106 117-56 55", "observation": "4 7"},
	{"map": "PX_DX_0", "doors": "16 5-106 117-45 44", "observation": "6 2"},
	{"map": "PX_NX_0", "doors": "64 65-106 117-67 66", "observation": "4 7"},
	{"map": "PX_PX_0", "doors": "17 6-105 116-34 33", "observation": "4 4"},
	{"map": "PX_UN_0", "doors": "53 54-97 98-56 55", "observation": "6 7"},
	{"map": "UN_DX_0", "doors": "16 5-56 55", "observation": "6 5"},
	{"map": "UN_NX_0", "doors": "18 7-78 77", "observation": "7 7"},
	{"map": "UN_PX_0", "doors": "75 76-105 116", "observation": "5 3"},
	{"map": "UN_UN_0", "doors": "64 65-104 115", "observation": "5 5"}
]

# Transform the participant-generated paths (a sequence of states) into a 
# sequence of actions.
def get_action_sequence(state_sequence):
	action_sequence = []
	for s in range(1, len(state_sequence)):
		differential = state_sequence[s] - state_sequence[s-1]
		if differential == -1:
			action_sequence.append(0)
		elif differential == 1:
			action_sequence.append(1)
		elif differential == -11:
			action_sequence.append(2)
		elif differential == 11:
			action_sequence.append(3)
	return action_sequence

# Transform x- and y-coordinates into a state representation.
def transform_state(agent, coords):
	return (coords[0]*agent.Plr.Map.mapwidth) + coords[1]

# Import the participant-generated paths.
data_0 = pd.read_csv("data/experiment_2/human/data_0/"+PARTICIPANT+".csv")

# Open a file for saving the results. Analysis and plotting will be done in R.
file = open("data/experiment_2/model/"+PARTICIPANT+".csv", "w")
writer = csv.writer(file, delimiter=",", lineterminator="\n")
writer.writerow(["participant", "map", "bayes_factor"])

# Iterate through each participant (if we have multiple) and compute the 
# Bayes factor of their paths on each trial.
for participant in data_0["participant"].unique():
	data_1 = data_0[data_0["participant"].isin([participant])]
	for world in data_1["map"]:
		# Let the user know which participant we're on.
		print("Participant #: "+str(participant)+"\t"+"Map: "+world)

		# Create an agent for the doors-to-observation (DO) map and the 
		# doors-to-goal map (while suppresing print output).
		sys.stdout = open(os.devnull, "w")
		agent_DO = LoadObserver("stimuli/experiment_2/"+world+"_DO", \
			Silent=True)
		agent = LoadObserver("stimuli/experiment_2/"+world, Silent=True)
		sys.stdout = sys.__stdout__

		# Retrieve the door location(s) and the observation and decode them.
		encoded_doors = [stimulus for stimulus in STIMULI \
			if stimulus["map"] == world][0]["doors"]
		doors = [[int(num) for num in pair.split(" ")] \
			for pair in encoded_doors.split("-")]
		encoded_observation = [stimulus for stimulus in STIMULI \
			if stimulus["map"] == world][0]["observation"]
		observation = transform_state(agent, \
			[int(num) for num in encoded_observation.split(" ")])

		# Retrieve the participant-generated path for this world (a sequence 
		# of states), decode it, and append the standard start state (to 
		# match the paths generated by our model).
		data_2 = data_1[data_1["map"].isin([world])]
		encoded_state_sequence = \
			[int(num) for num in data_2["coords"].iloc[0].split(",")]
		state_sequence = []
		for i in range(len(encoded_state_sequence)/2):
			state_sequence.append(transform_state(agent,
				[encoded_state_sequence[1::2][i], \
				encoded_state_sequence[::2][i]]))
		if state_sequence[0] in np.linspace(24, 90, 7):
			start_state = state_sequence[0] - 1
			end_state = state_sequence[0] - 2
		elif state_sequence[0] in np.linspace(30, 96, 7):
			start_state = state_sequence[0] + 1
			end_state = state_sequence[0] + 2
		elif state_sequence[0] in np.linspace(24, 30, 7):
			start_state = state_sequence[0] - 11
			end_state = state_sequence[0] - 22
		elif state_sequence[0] in np.linspace(90, 96, 7):
			start_state = state_sequence[0] + 11
			end_state = state_sequence[0] + 22
		state_sequence = [start_state] + state_sequence

		# Split the state sequence into a door-to-observation (DO) state 
		# sequence and an observation-to-goal (OG) state sequence. 
		state_sequence_DO = []
		for i in range(len(state_sequence)):
			state_sequence_DO.append(state_sequence[i])
			if state_sequence[i] == observation:
				break
		state_sequence_OG = state_sequence[i:]

		# Convert the state sequences into an action sequences.
		action_sequence_DO = get_action_sequence(state_sequence_DO)
		action_sequence_OG = get_action_sequence(state_sequence_OG)

		# Compute the Bayes factor for this trial while uniformly sampling 
		# reward functions.
		likelihoods = []
		for i in range(NUM_SAMPLES):
			# Let the user know which sample we're on.
			print("Sample #: "+str(i))

			# Sample which door both agents use.
			door = random.choice(doors)
			agent_DO.SetStartingPoint(door[0], Verbose=False)
			agent.SetStartingPoint(door[0], Verbose=False)
			agent_DO.Plr.Map.ExitState = door[1]
			agent.Plr.Map.ExitState = door[1]

			# Sample a reward function for each agent. This doesn't matter 
			# for the doors-to-observation (DO) agent since it only has one 
			# goal.
			agent_DO.Plr.Agent.ResampleAgent()
			agent.Plr.Agent.ResampleAgent()

			# Run the planner for each agent (while supressing print output).
			sys.stdout = open(os.devnull, "w")
			agent_DO.Plr.Prepare()
			agent.Plr.Prepare()
			sys.stdout = sys.__stdout__

			# Compute the likelihood of the participant-generated path (from 
			# the door to the observation).
			log_likelihood_DO = 0
			for i in range(len(action_sequence_DO)):
				log_likelihood_DO += np.log(agent_DO.Plr.Policies[1] \
					[action_sequence_DO[i]][state_sequence_DO[i]])

			# Find which of the three objects the participant-generated path 
			# went to.
			goal_index = \
				agent.Plr.CriticalStates.index(state_sequence_OG[-1]) - 1

			# Compute the softmaxed action probabilities.
			utilities = agent.Plr.Utilities
			utilities = utilities - abs(max(utilities))
			try:
				action_probabilities = \
					[np.exp(utilities[j]/agent.Plr.Agent.choiceTau) \
					for j in range(len(utilities))]
			except OverflowError:
				action_probabilities = utilities
				print("ERROR: Failed to softmax utility function.")
			if sum(utilities) == 0:
				# If none of the actions have a high probability, perhaps 
				# because they all have high negative utilities, assign an 
				# equal action probability to each of them.
				action_probabilities = [1.0/len(utilities)] * \
					len(utilities)
			else:
				# Otherwise, normalize the action probabilities we've 
				# computed.
				action_probabilities = \
					[action_probabilities[j]/sum(action_probabilities) \
					for j in range(len(action_probabilities))]

			# Compute the likelihood of the participant-generated path (from 
			# the observation to the goals).
			log_likelihood_OG = 0
			for i in range(len(action_sequence_OG)):
				log_likelihood_OG += np.log(agent.Plr.Policies[goal_index+1] \
					[action_sequence_OG[i]][state_sequence_OG[i]])

			# Add the likelihood of each path together.
			log_likelihood = log_likelihood_DO + log_likelihood_OG

			# If the action probability of the goal the participant went for 
			# is greater than 0, then add the log likelihood of selecting 
			# that goal to our log likelihood estimate. Otherwise, the set it 
			# to 0.
			if action_probabilities[goal_index] > 0:
				log_likelihood += np.log(action_probabilities[goal_index])
			else:
				log_likelihood = (-sys.maxint - 1)
				
			# Convert the log likelihood back into a probability.
			likelihood = np.exp(log_likelihood)
			likelihoods.append(likelihood)

		# Average the likelihoods over a uniform distribution of reward 
		# functions.
		model_probability = np.mean(likelihoods)

		# Compute the probability that the alternative model would generate 
		# this path.
		alternative_probability = np.power(0.33, len(state_sequence)-1)
		
		# Compute the Bayes factor.
		bayes_factor = model_probability / alternative_probability
		writer.writerow([participant, world, bayes_factor])

# Close the file.
file.close()