---
title: "Image Inference - Experiment 1"
output:
  html_document:
    code_folding: hide
    df_print: paged
  pdf_document: default
---

```{r setup, echo=FALSE}
# Set the working directory.
knitr::opts_knit$set(root.dir=normalizePath("../.."))

# Turn off compile messages and warnings.
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

# Set the seed.
set.seed(0)

# Set up the path to the participant data.
human_path = "data/experiment_1/human/data_0"

# Set up the path to the model data.
model_path = "data/experiment_1/model/predictions/Manhattan"
```

```{r libraries, echo=FALSE}
# Import R libraries.
library(boot)
library(jsonlite)
library(nnet)
library(tidyverse)
```

# Preprocessing

```{r preprocessing}
# Read in the participant data (after manually removing errors).
data_0 <- read_csv(file.path(human_path, "raw_data.csv"), quote="~")

# Read in the MTurk results file.
mturk_results = read_csv(file.path(human_path, "mturk_results.csv"), col_names=TRUE) %>%
  mutate(Answer.surveycode=substr(Answer.surveycode, 3, length(Answer.surveycode))) %>%
  filter(AssignmentStatus=="Approved")

# Filter out any duplicate database entries based on the MTurk results file.
duplicates <- setdiff(data_0$unique_id, mturk_results$Answer.surveycode)
if (length(duplicates) != 0) { stop("There are duplicate entries.") }

# Convert the JSON string into JSON.
data_1 = lapply(data_0$results, fromJSON)

# Extract the trial information for each participant and stack them.
data_3 = tibble()
for (p in 1:length(data_1)) {
  # Trim the map and add the participant ID back in.
  data_2 = data_1[p][[1]]$trials %>%
    as.data.frame() %>%
    mutate(map=gsub(".png", "", map), unique_id=data_0$unique_id[p],
           wrong_attempts=data_1[p][[1]]$catch_trials$wrong_attempts,
           age=data_1[p][[1]]$subject_information$age)

  # Stack the trial information for the current participant.
  data_3 = rbind(data_3, data_2)
}

# Write the preprocessed data.
write_csv(data_3, file.path(human_path, "data.csv"))
```

# Goal Inference

First, we generate a scatter plot comparing our model predictions with participant judgments on the goal inferences.

```{r goal_inference, fig.align="center"}
# Read in the preprocessed data.
data_3 = read_csv(file.path(human_path, "data.csv"))

# Select and normalize the goal judgments.
data_4 = data_3 %>%
  select(unique_id, map, A, B, C) %>%
  gather(goal, human, A, B, C) %>%
  left_join(do(., summarize(group_by(., unique_id, map),
                            total_human=sum(human)))) %>%
  mutate(human=human/total_human) %>%
  select(-total_human)

# Define the bootstrap function for the bootstrap statistic.
compute_mean = function(data, indices) {
  return(mean(data[indices]))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_mean,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CIs.
ci = data.frame()
for (m in unique(data_4$map)) {
    # Compute the bootstrap for each dependent measure.
    A_bootstrap = compute_bootstrap(filter(data_4, map==m, goal=="A")$human)
    B_bootstrap = compute_bootstrap(filter(data_4, map==m, goal=="B")$human)
    C_bootstrap = compute_bootstrap(filter(data_4, map==m, goal=="C")$human)
    
    # Store the bootstrapped 95% CIs for this pair.
    ci = rbind(ci, data.frame(map=rep(m, 3),
                              goal=c("A", "B", "C"),
                              lower=c(A_bootstrap[4], B_bootstrap[4], C_bootstrap[4]),
                              upper=c(A_bootstrap[5], B_bootstrap[5], C_bootstrap[5])))
}

# Read in the goal predictions.
model_0 = read_csv(file.path(model_path, "goal_predictions.csv"))

# Perform some basic preprocessing and then z-score the model predictions.
model_1 = model_0 %>%
  select(-Goal) %>%
  rename(goal=GoalLetter, map=Trial, model=Probability) %>%
  mutate(z_model=scale(model))

# Merge the z-scored participant judgments with the bootstrapped 95% CIs and the model predictions.
data_5 = data_4 %>%
  group_by(unique_id) %>%
  mutate(z_human=scale(human)) %>%
  ungroup() %>%
  group_by(map, goal) %>%
  summarize(mean_human=mean(human), mean_z_human=mean(z_human)) %>%
  left_join(ci) %>%
  left_join(model_1)

# Plot the goal comparison.
plot_0 = data_5 %>%
  ggplot(aes(x=model, y=mean_human, label=map)) +
  geom_point(aes(color=substr(map, 0, 2))) +
  # geom_text(vjust=-1.0) +
  geom_smooth(method="lm", se=FALSE, linetype="dashed", color="grey") +
  ggtitle("Model Goal Predictions vs. Participant Goal Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  scale_color_discrete(name="Goal") + 
  theme_classic() +
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))
plot_0
```

```{r goal_correlation, echo=FALSE}
# Define the bootstrap function for the bootstrap statistic.
compute_cor = function(data, indices) {
  return(cor(data$model[indices], data$mean_human[indices], method="pearson"))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the correlation and its bootstrapped 95% CI.
cor_0 = cor(data_5$model, data_5$mean_human, method="pearson")
cor_0_bootstrap = compute_bootstrap(data_5)
cor_0_ci = data.frame(
  lower=cor_0_bootstrap[4],
  upper=cor_0_bootstrap[5]
)
```

The Pearson correlation is _r_ = `r round(cor_0, 2)` (95% CI: `r round(cor_0_ci$lower, 2)`-`r round(cor_0_ci$upper, 2)`). Next, we generate comparisons for each trial.

```{r goal_inference_per_trial, fig.align="center"}
# Plot goal inferences by trial.
plot_1 = data_5 %>%
  gather(type, value, mean_human, model) %>%
  ggplot(aes(x=goal, y=value, group=type)) +
  geom_point(aes(color=type)) +
  geom_line(aes(color=type)) + 
  geom_errorbar(aes(ymin=lower, ymax=upper), color="#F8766D", width=0.2) +
  facet_wrap(~map) +
  xlab("Goal") +
  ylab("Value") +
  scale_color_discrete(name="Type",
                       limits=c("mean_human", "model"),
                       labels=c("Human", "Model")) +
  theme_classic() +
  theme(aspect.ratio=1.0,
        legend.title=element_text(hjust=0.5))
plot_1
```

# Entrance Inference

First, we generate a scatter plot comparing our model predictions with participant judgments on the entrance inferences.

```{r entrance_inference, fig.align="center"}
# Select and normalize the entrance judgments.
data_6 = data_3 %>%
  filter(!grepl("ND", map)) %>%
  select(unique_id, map, `1`, `2`, `3`) %>%
  gather(entrance, human, `1`, `2`, `3`) %>%
  filter(human!=-1) %>%
  left_join(do(., summarize(group_by(., unique_id, map),
                            total_human=sum(human)))) %>%
  mutate(human=human/total_human) %>%
  select(-total_human)

# Define the bootstrap function for the bootstrap statistic.
compute_mean = function(data, indices) {
  return(mean(data[indices]))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_mean,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CIs.
ci = data.frame()
for (m in unique(data_6$map)) {
    # Compute the bootstrap for each dependent measure.
    bootstrap_1 = compute_bootstrap(filter(data_6, map==m, entrance=="1")$human)
    bootstrap_2 = compute_bootstrap(filter(data_6, map==m, entrance=="2")$human)
    if (length(unique(filter(data_6, map==m)$entrance)) == 3) {
      bootstrap_3 = compute_bootstrap(filter(data_6, map==m, entrance=="3")$human)
    }
    
    # Store the bootstrapped 95% CIs for this pair.
    if (length(unique(filter(data_6, map==m)$entrance)) == 3) {
      ci = rbind(ci, data.frame(map=rep(m, 3),
                                entrance=c("1", "2", "3"),
                                lower=c(bootstrap_1[4], bootstrap_2[4], bootstrap_3[4]),
                                upper=c(bootstrap_1[5], bootstrap_2[5], bootstrap_3[5])))
    }
    else {
      ci = rbind(ci, data.frame(map=rep(m, 2),
                                entrance=c("1", "2"),
                                lower=c(bootstrap_1[4], bootstrap_2[4]),
                                upper=c(bootstrap_1[5], bootstrap_2[5])))
    }
}

# Read in the entrance predictions.
model_2 = read_csv(file.path(model_path, "entrance_predictions.csv"))

# Stitch the entrance mapping to the predictions and do some preprocessing.
model_3 = model_2 %>%
  right_join(read_csv(file.path(model_path, "entrance_mapping.csv"))) %>%
  mutate(Probability=ifelse(is.na(Probability), 0, Probability)) %>%
  select(-Entrance) %>%
  rename(map=TrialName, entrance=DoorNumber, model=Probability) %>%
  mutate(entrance=as.character(entrance), z_model=scale(model))

# Select the entrance judgments, norm them, then z-score within participants.
data_7 = data_6 %>%
  group_by(unique_id) %>%
  mutate(z_human=scale(human)) %>%
  ungroup() %>%
  group_by(map, entrance) %>%
  summarize(mean_human=mean(human), mean_z_human=mean(z_human)) %>%
  left_join(ci) %>%
  left_join(model_3)

# Plot the entrance comparison.
plot_2 = data_7 %>%
  ggplot(aes(x=model, y=mean_human, label=map)) +
  geom_point(aes(color=substr(map, 0, 2))) +
  # geom_text(vjust=-1.0) +
  geom_smooth(method="lm", se=FALSE, linetype="dashed", color="grey") +
  ggtitle("Model Entrance Predictions vs. Participant Entrance Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  scale_color_discrete(name="Entrance") +
  theme_classic() + 
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))
plot_2
```

```{r entrance_correlation, echo=FALSE}
# Define the bootstrap function for the bootstrap statistic.
compute_cor = function(data, indices) {
  return(cor(data$model[indices], data$mean_human[indices], method="pearson"))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CI.
cor_1 = cor(data_7$model, data_7$mean_human, method="pearson")
cor_1_bootstrap = compute_bootstrap(data_5)
cor_1_ci = data.frame(
  lower=cor_1_bootstrap[4],
  upper=cor_1_bootstrap[5]
)
```

The Pearson correlation is _r_ = `r round(cor_1, 2)` (95% CI: `r round(cor_1_ci$lower, 2)`-`r round(cor_1_ci$upper, 2)`). Next, we generate comparisons for each trial.

```{r entrance_inference_per_trial, fig.align="center"}
# Plot entrance inferences by trial.
plot_3 = data_7 %>%
  gather(type, value, mean_human, model) %>%
  ggplot(aes(x=entrance, y=value, group=type)) +
  geom_point(aes(color=type)) +
  geom_line(aes(color=type)) +
  geom_errorbar(aes(ymin=lower, ymax=upper), color="#F8766D", width=0.2) +
  facet_wrap(~map) +
  xlab("Entrance") +
  ylab("Value") +
  scale_color_discrete(name="Type",
                       limits=c("mean_human", "model"),
                       labels=c("Human", "Model")) +
  theme_classic() +
  theme(aspect.ratio=1.0,
        legend.title=element_text(hjust=0.5))
plot_3
```

# Combined Inference

Finally, we can combine both scatter plots.

```{r combined_inference, fig.align="center"}
# Merge the two inferences.
data_8 = data_5 %>%
  mutate(type="goal", inference=goal) %>%
  select(-goal) %>%
  rbind(select(mutate(data_7, type="entrance", inference=entrance), -entrance))

plot_4 = data_8 %>%
  ggplot(aes(x=model, y=mean_human, label=map)) +
  geom_point(aes(color=type)) +
  # geom_text(vjust=-1.0, hjust=-1.0) +
  geom_smooth(method="lm", se=FALSE, linetype="dashed", color="grey") +
  ggtitle("Combined Model Predictions vs. Participant Judgments") +
  xlab("Model Predictions") +
  ylab("Participant Judgments") +
  scale_color_discrete(name="Type",
                       limits=c("entrance", "goal"),
                       labels=c("Entrance", "Goal")) +
  theme_classic() +
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))
plot_4
```

```{r combined_correlation, echo=FALSE}
# Define the bootstrap function for the bootstrap statistic.
compute_cor = function(data, indices) {
  return(cor(data$model[indices], data$mean_human[indices], method="pearson"))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CI.
cor_2 = cor(data_8$model, data_8$mean_human, method="pearson")
cor_2_bootstrap = compute_bootstrap(data_8)
cor_2_ci = data.frame(
  lower=cor_2_bootstrap[4],
  upper=cor_2_bootstrap[5]
)
```

The Pearson correlation is _r_ = `r round(cor_2, 2)` (95% CI: `r round(cor_2_ci$lower, 2)`-`r round(cor_2_ci$upper, 2)`).

# Alternative Models

## Predicting model error as a function of map complexity

Now we test if we can predict model error as a function of how many doors there are in each map. First, we generate a scatter plot of the goal inferences that colors each point based on the number of entrances.

```{r goal_inference_by_entrance_count, fig.align="center"}
plot_5 = data_5 %>%
  mutate(num_entrances=ifelse(grepl("ND", substr(map, 1, 2)), 1,
                              ifelse(grepl("DX|NX", substr(map, 1, 2)), 2, 3))) %>%
  ggplot(aes(x=model, y=mean_human, label=map)) +
  geom_point(aes(color=factor(num_entrances))) +
  # geom_text(vjust=-1.0) +
  geom_smooth(method="lm", se=FALSE, linetype="dashed", color="grey") +
  ggtitle("Model Goal Predictions vs. Participant Goal Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  scale_color_discrete(name="Number of Entrances") + 
  theme_classic() +
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))
plot_5
```

There doesn't seem to be anything here, so let's compute a linear regression that predicts model error (on the goal inferences) as a function of the number of entrances in each map. 

```{r alternative_model_goal_error}
data_9 = data_4 %>%
  left_join(model_1) %>%
  mutate(model_error=human-model,
         num_entrances=ifelse(grepl("ND", substr(map, 1, 2)), 1,
                          ifelse(grepl("DX|NX", substr(map, 1, 2)), 2, 3)))
alternative_0 = lm(formula=model_error~num_entrances, data=data_9)
summary(alternative_0)
```

Next, we generate a scatter plot of the entrance inferences that colors each point based on the number of entrances.

```{r entrance_inference_by_entrance_count, fig.align="center"}
plot_6 = data_7 %>%
  mutate(num_entrances=ifelse(grepl("ND", substr(map, 1, 2)), 1,
                              ifelse(grepl("DX|NX", substr(map, 1, 2)), 2, 3))) %>%
  ggplot(aes(x=model, y=mean_human, label=map)) +
  geom_point(aes(color=factor(num_entrances))) +
  # geom_text(vjust=-1.0) +
  geom_smooth(method="lm", se=FALSE, linetype="dashed", color="grey") +
  ggtitle("Model Entrance Predictions vs. Participant Entrance Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  scale_color_discrete(name="Entrance") +
  theme_classic() + 
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))
plot_6
```

There also doesn't seem to be anything here, so let's compute another linear regression that attempts to predict model error (on the entrance inferences) as a function of the number of entrances.

```{r alternative_model_entrance_error}
data_10 = data_6 %>%
  left_join(model_3) %>%
  mutate(model_error=human-model,
         num_entrances=ifelse(grepl("ND", substr(map, 1, 2)), 1,
                          ifelse(grepl("DX|NX", substr(map, 1, 2)), 2, 3)))
alternative_1 = lm(formula=model_error~num_entrances, data=data_10)
summary(alternative_1)
```

## Predicting goal inference as a function of goal and entrance distance

Now we test whether an alternative model that uses simple distance heuristics performs better than our model. This alternative model consists of a multinomial logistic regression that attempts to predict which goal the agent was going for using (1) the distance between the observation and each goal, (2) the distance between the observation and the nearest entrance, and (3) the interactions between (1) and (2).

```{r alternative_model_distance_heuristics}
# Read in the distances between the observation and each goal and entrance.
data_11 = read_csv("data/experiment_1/alternative/distances.csv")

# Create three binary columns that indicate which goal (A, B, or C) was judged to be the most likely goal.
data_12 = data_5 %>%
  select(map, goal, mean_human) %>%
  spread(goal, mean_human) %>%
  left_join(data_11)
data_14 = tibble()
for (m in data_12$map) {
  data_13 = data_12 %>%
    filter(map==m) %>%
    mutate(max_goal=ifelse(max(A, B, C)==A, 1,
                           ifelse(max(A, B, C)==B, 2, 3)))
  data_14 = bind_rows(data_14, data_13)
}

# Set the baseline variable.
data_14$max_goal = factor(data_14$max_goal)  
data_14$max_goal = relevel(data_14$max_goal, ref=1)

# Compute the multinomial logistic regression.
alternative_2 = multinom(formula=max_goal~A_d+B_d+C_d+`1_d`+A_d:`1_d`+B_d:`1_d`+C_d:`1_d`,
                         data=data_14)
summary(alternative_2)
```

```{r model_comparison}
# Stitch the alternative model predictions to the mean participant judgments and our model predictions.
data_15 = data_5 %>%
  select(map, goal, mean_human, model) %>%
  left_join(., gather(data.frame(map=data_14$map,
                                 A=fitted(alternative_2)[,1],
                                 B=fitted(alternative_2)[,2],
                                 C=fitted(alternative_2)[,3]),
                      goal, alternative, A, B, C))

# Define the bootstrap function for the bootstrap statistic.
compute_cor = function(data, indices) {
  return(cor(data$alternative[indices], data$mean_human[indices], method="pearson"))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CI.
cor_3 = cor(data_15$alternative, data_15$mean_human, method="pearson")
cor_3_bootstrap = compute_bootstrap(data_15)
cor_3_ci = data.frame(
  lower=cor_3_bootstrap[4],
  upper=cor_3_bootstrap[5]
)

# Define the bootstrap function for the bootstrap statistic.
compute_cor_diff = function(data, indices) {
  cor_alternative = cor(data$alternative[indices], data$mean_human[indices], method="pearson")
  cor_model = cor(data$model[indices], data$mean_human[indices], method="pearson")
  return(cor_alternative-cor_model)
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor_diff,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the correlation difference and the bootstrapped 95% CI.
cor_4 = cor_3 - cor_2
cor_4_bootstrap = compute_bootstrap(data_15)
cor_4_ci = data.frame(
  lower=cor_4_bootstrap[4],
  upper=cor_4_bootstrap[5]
)
```

The Pearson correlation of the alternative model is _r_ = `r round(cor_3, 2)` (95% CI: `r round(cor_3_ci$lower, 2)`-`r round(cor_3_ci$upper, 2)`). The correlation difference between the alternative model and our model is $\Delta$_r_ = `r abs(round(cor_4, 2))` (95% CI: `r abs(round(cor_4_ci$upper, 2))`-`r abs(round(cor_4_ci$lower, 2))`).
