---
title: "Image Inference - Experiment 1"
output:
  html_document:
    code_folding: hide
    df_print: paged
  pdf_document: default
---

```{r setup, echo=FALSE}
# Set the working directory.
path <- "D:/Research/ImageInference/"
knitr::opts_knit$set(root.dir=normalizePath("../.."))

# Turn off compile messages and warnings.
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```

```{r, echo=FALSE}
# Import R libraries.
library(jsonlite)
library(tidyverse)
```

# pilot_0

Some of the supplemental information collected from participants was lost due to lack of sanitizing inputs:

- Participant 5YIBOVS: Questionnaire cut off after "problems". Reconstructing form with blank entries.
- Participant Gx3kOR2: Questionnaire cut off in the middle of their response in "problems". Ending response at current location and reconstructing form with blank entries.
- Participant 9UhOHcJ: Questionnaire cut off in the middle of their response in "problems". Ending response at current location and reconstructing form with blank entries.

```{r, echo=FALSE}
# Read in the participant data (after manually removing errors).
data_0 <- read_csv("data/experiment_1/pilot_0/clean_data.csv", quote="~")

# Read in the MTurk results file.
mturk_results = read_csv("data/experiment_1/pilot_0/mturk_results.csv", col_names=TRUE) %>%
  mutate(Answer.surveycode=substr(Answer.surveycode, 3, length(Answer.surveycode)))

# Filter out any duplicate database entries based on the MTurk results file.
duplicates <- setdiff(data_0$UniqueId, mturk_results$Answer.surveycode)
data_1 <- data_0 %>%
  filter(!(UniqueId %in% duplicates))

# Convert the JSON string into JSON.
data_2 = lapply(data_1$Results, fromJSON)

# Extract the trial information for each participant and stack them.
data_4 = tibble()
for (p in 1:length(data_2)) {
  # Trim the layout and add the participant ID back in.
  data_3 = data_2[p][[1]]$trials %>%
    as.data.frame() %>%
    mutate(layout=gsub("_no-grid.png", "", layout), unique_id=data_1$UniqueId[p])

  # Stack the trial information for the current participant.
  data_4 = rbind(data_4, data_3)
}

# Write the preprocessed data.
# write_csv(data_4, "data/experiment_1/pilot_0/data.csv")
```

## Goal Predictions

```{r, fig.align="center"}
# Read in the goal judgments and predictions.
model_0 = read_csv("data/model/predictions/Manhattan/goal_predictions.csv")

# Perform some basic preprocessing and then z-score the model predictions.
model_1 = model_0 %>%
  select(-Goal) %>%
  rename(goal=GoalLetter, layout=Trial, model=Probability) %>%
  mutate(z_model=scale(model))

# Select the goal judgments, norm them, then z-score within participants.
data_5 = data_4 %>%
  select(unique_id, layout, A, B, C) %>%
  mutate(norm_A=A/(A+B+C), norm_B=B/(A+B+C), norm_C=C/(A+B+C)) %>%
  select(-A, -B, -C) %>%
  rename(A=norm_A, B=norm_B, C=norm_C) %>%
  gather(goal, human, A, B, C) %>%
  group_by(unique_id) %>%
  mutate(z_human=scale(human)) %>%
  ungroup() %>%
  group_by(layout, goal) %>%
  summarize(mean_human=mean(human), mean_z_human=mean(z_human)) %>%
  left_join(model_1)

# Plot the goal comparison.
data_5 %>%  
  ggplot(aes(x=z_model, y=mean_z_human, label=layout)) +
  geom_point() +
  geom_smooth(method="lm") +
  ggtitle("Model Goal Predictions vs. Participant Goal Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5))
```

The correlation is `r cor(data_5$z_model, data_5$mean_z_human, method="pearson")`. If we break down the results by goal, we get:

```{r, fig.align="center"}
data_5 %>%
  ggplot(aes(x=z_model, y=mean_z_human, label=layout)) +
  geom_point() +
  geom_smooth(method="lm") +
  facet_wrap(~goal) +
  ggtitle("Model Goal Predictions vs. Participant Goal Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5))
```

## Entrance Predictions

```{r, fig.align="center"}
# Read in the entrance predictions.
model_0 = read_csv("data/model/predictions/Manhattan/entrance_predictions.csv")

# Stitch the entrance mapping to the predictions and do some preprocessing.
model_1 = model_0 %>%
  left_join(read_csv("data/model/predictions/entrance_mapping.csv")) %>%
  mutate(Probability=ifelse(is.na(Probability), 0, Probability)) %>%
  select(-Entrance) %>%
  rename(layout=TrialName, entrance=DoorNumber, model=Probability) %>%
  mutate(entrance=as.character(entrance), z_model=scale(model))

# Select the entrance judgments, norm them, then z-score within participants.
data_6 = data_4 %>%
  select(unique_id, layout, `1`, `2`, `3`) %>%
  gather(entrance, human, `1`, `2`, `3`) %>%
  mutate(human=ifelse(human==-1, 0, human)) %>%
  spread(entrance, human) %>%
  mutate(norm_1=`1`/(`1`+`2`+`3`), norm_2=`2`/(`1`+`2`+`3`), norm_3=`3`/(`1`+`2`+`3`)) %>%
  select(-`1`, -`2`, -`3`) %>%
  rename(`1`=norm_1, `2`=norm_2, `3`=norm_3) %>%
  gather(entrance, human, `1`, `2`, `3`) %>%
  group_by(unique_id) %>%
  mutate(z_human=scale(human)) %>%
  ungroup() %>%
  group_by(layout, entrance) %>%
  summarize(mean_human=mean(human), mean_z_human=mean(z_human)) %>%
  left_join(model_1)

# Omit missing data from trials that had only 1 or 2 entrances.
data_7 = data_6 %>%
  na.omit()

# Plot the entrance comparison.
data_7 %>%
  ggplot(aes(x=z_model, y=mean_z_human, label=layout)) +
  geom_point() +
  geom_smooth(method="lm") +
  ggtitle("Model Entrance Predictions vs. Participant Entrance Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5))
```

The Pearson correlation is `r cor(data_7$z_model, data_7$mean_z_human, method="pearson")`.If we break down the results by entrance, we get:

```{r, fig.align="center"}
data_7 %>%
  ggplot(aes(x=z_model, y=mean_z_human, label=layout)) +
  geom_point() +
  geom_smooth(method="lm") +
  facet_wrap(~entrance) +
  ggtitle("Model Entrance Predictions vs. Participant Entrance Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5))
```

## Debugging

```{r, echo=FALSE}
# Extract the maps where the model predictions and human judgments differ
# by more than 1 standard deviation.
unique(data_5$layout[abs(data_5$mean_z_human-data_5$z_model[,1]) > 1])
```