---
title: "Image Inference - Experiment 3"
output:
  html_document:
    code_folding: hide
    df_print: paged
  pdf_document: default
---
  
```{r setup, echo=FALSE}
# Set the working directory.
knitr::opts_knit$set(root.dir=normalizePath("../.."))

# Turn off compile messages and warnings.
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

# Set the seed value.
seed = 0

# Set up which data iteration we're analyzing.
data = "data_0"

# Set up the path to the participant data.
human_path = file.path("data/experiment_3/human/", data)

# Set up the path to the model data.
model_path = "data/experiment_3/model/predictions/Manhattan"
```

```{r libraries, echo=FALSE}
# Import R libraries.
library(boot)
library(cowplot)
library(jsonlite)
library(tidyverse)
```

# Preprocessing

```{r preprocessing}
# Read in the participant data (after manually removing errors).
data_0 = read_csv(file.path(human_path, "raw_data.csv"), quote="~")

# Read in the MTurk results file.
mturk_results = read_csv(file.path(human_path, "mturk_results.csv"), col_names=TRUE) %>%
  mutate(Answer.surveycode=substr(Answer.surveycode, 3, length(Answer.surveycode))) %>%
  filter(AssignmentStatus=="Approved")

# Stop if there are duplicate database entries based on the MTurk results file.
duplicates <- setdiff(data_0$unique_id, mturk_results$Answer.surveycode)
if (length(duplicates) != 0) { stop("There are duplicate entries.") }

# Convert the JSON string into JSON.
data_1 = lapply(data_0$results, fromJSON)

# Extract the trial information for each participant and stack them.
age = c()
data_3 = tibble()
for (p in 1:length(data_1)) {
  # Trim the map and add the participant ID back in.
  data_2 = data_1[p][[1]]$trials %>%
    as.data.frame() %>%
    mutate(map=gsub(".png", "", map), unique_id=data_0$unique_id[p],
           wrong_attempts=data_1[p][[1]]$catch_trials$wrong_attempts)

  # Extract and store this participant's age.
  age = c(age, as.integer(data_1[p][[1]]$subject_information$age))
  
  # Stack the trial information for the current participant.
  data_3 = rbind(data_3, data_2)
}

# Write the preprocessed data.
write_csv(data_3, file.path(human_path, "data.csv"))
```

# Number of Agents Inference

```{r num_agents_inference, fig.align="center"}
# Read in the preprocessed data.
data_3 = read_csv(file.path(human_path, "data.csv"))

# Compute the z-scored participant judgmnets.
data_4 = data_3 %>%
  group_by(unique_id) %>%
  mutate(z_num_agents=scale(num_agents)[,1]) %>%
  ungroup()

# Define the bootstrap function for the bootstrap statistic.
compute_mean = function(data, indices) {
  return(mean(data[indices]))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_mean,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CIs.
set.seed(seed)
ci = data.frame()
for (m in unique(data_3$map)) {
    # Compute the bootstrap for the dependent measure.
    num_agents_bootstrap = compute_bootstrap(filter(data_4, map==m)$num_agents)
    z_num_agents_bootstrap = compute_bootstrap(filter(data_4, map==m)$z_num_agents)
    
    # Store the bootstrapped 95% CIs.
    ci = rbind(ci, data.frame(map=m,
                              lower=num_agents_bootstrap[4],
                              upper=num_agents_bootstrap[5],
                              z_lower=z_num_agents_bootstrap[4],
                              z_upper=z_num_agents_bootstrap[5]))
}

# Read in the goal predictions.
model_0 = read_csv(file.path(model_path, "one_goal_per_path/data.csv"))

# Perform some basic preprocessing and then z-score the model predictions.
model_1 = model_0 %>%
  select(-l_1, -l_2) %>%
  rename(model=p_2) %>%
  mutate(z_model=scale(model)[,1])

# Merge the non-standardized and standardized participant judgments with the 
# bootstrapped 95% CIs and the model predictions.
data_5 = data_4 %>%
  rename(human=num_agents, z_human=z_num_agents) %>%
  group_by(map) %>%
  summarize(mean_human=mean(human), mean_z_human=mean(z_human)) %>%
  left_join(ci) %>%
  left_join(model_1)

# Generate a plot of the goal comparison using the non-standardized model 
# predictions and mean participant judgments.
plot_0 = data_5 %>%
  ggplot(aes(x=model, y=mean_human, label=map)) +
  geom_point(aes(color=substr(map, 0, 2))) +
  # geom_text(vjust=-1.0) +
  geom_smooth(method="lm", se=TRUE, linetype="dashed", color="black") +
  ggtitle("Model Agent Predictions vs.\nParticipant Agent Judgments") +
  xlab("Model Predictions") +
  ylab("Human Judgments") +
  scale_color_discrete(name="Map") +
  theme_classic() +
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))

# Generate a plot of the goal comparison using the standardized model 
# predictions and mean z-scored participant judgmnets.
plot_1 = data_5 %>%
  ggplot(aes(x=z_model, y=mean_z_human, label=map)) +
  geom_point(aes(color=substr(map, 0, 2))) +
  # geom_text(vjust=-1.0) +
  geom_smooth(method="lm", se=TRUE, linetype="dashed", color="black") +
  ggtitle("Model Agent Predictions vs.\nParticipant Agent Judgments") +
  xlab("Standardized Model Predictions") +
  ylab("Standardized Human Judgments") +
  scale_color_discrete(name="Map") +
  theme_classic() +
  theme(aspect.ratio=1.0,
        plot.title=element_text(hjust=0.5),
        legend.title=element_text(hjust=0.5))

# Combine the two plots.
plot_2 = plot_grid(plot_0, plot_1, nrow=1)
plot_2
```

```{r num_agents_correlation, echo=FALSE}
# Define the bootstrap function for the bootstrap statistic.
compute_cor = function(data, indices) {
  return(cor(data$model[indices], data$mean_human[indices], method="pearson"))
}

# Define the bootstrap function for the bootstrap statistic.
compute_z_cor = function(data, indices) {
  return(cor(data$z_model[indices], data$mean_z_human[indices], method="pearson"))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data, statistic) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=statistic,
                     R=10000)

  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the correlation between the non-standardized model predictions and 
# mean participant judgments.
set.seed(seed)
cor_0 = cor(data_5$model, data_5$mean_human, method="pearson")
cor_0_bootstrap = compute_bootstrap(data_5, compute_cor)
cor_0_ci = data.frame(
  lower=cor_0_bootstrap[4],
  upper=cor_0_bootstrap[5]
)

# Compute the correlation between the standardized model predictions and mean 
# participant judgments.
set.seed(seed)
cor_1 = cor(data_5$z_model, data_5$mean_z_human, method="pearson")
cor_1_bootstrap = compute_bootstrap(data_5, compute_z_cor)
cor_1_ci = data.frame(
  lower=cor_1_bootstrap[4],
  upper=cor_1_bootstrap[5]
)
```

The Pearson correlation using the the non-standardized data is _r_ = `r round(cor_0, 2)` (95% CI: `r round(cor_0_ci$lower, 2)`-`r round(cor_0_ci$upper, 2)`). The Pearson correlation using the standardized data is _r_ = `r round(cor_1, 2)` (95% CI: `r round(cor_1_ci$lower, 2)`-`r round(cor_1_ci$upper, 2)`). Next, we generate comparisons for each trial for both groups of data.

```{r num_agents_inference_per_trial, fig.align="center"}
# Generate a plot of agent inferences by trial using the non-standardized 
# model predictions and mean participant judgments.
plot_3 = data_5 %>%
  gather(type, value, mean_human, model) %>%
  mutate(lower=ifelse(type=="model", NA, lower),
         upper=ifelse(type=="model", NA, upper)) %>%
  ggplot(aes(x=type, y=value)) +
  geom_bar(aes(fill=type), color="black", stat="identity") +
  geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2) +
  facet_wrap(~map) +
  scale_x_discrete(name="Type",
                   limits=c("mean_human", "model"),
                   labels=c("Human", "Model")) +
  ylab("Probability of Two Agents") +
  theme_classic() +
  theme(aspect.ratio=1.0,
        legend.position="none",
        axis.text.x=element_text(hjust=1.0, angle=45))

# Generate a plot of agent inferences by trial using the standardized model 
# predictions and mean participant judgments.
plot_4 = data_5 %>%
  gather(type, value, mean_z_human, z_model) %>%
  mutate(z_lower=ifelse(type=="z_model", NA, z_lower),
         z_upper=ifelse(type=="z_model", NA, z_upper)) %>%
  ggplot(aes(x=type, y=value)) +
  geom_bar(aes(fill=type), color="black", stat="identity") +
  geom_errorbar(aes(ymin=z_lower, ymax=z_upper), width=0.2) +
  facet_wrap(~map) +
  scale_x_discrete(name="Type",
                   limits=c("mean_z_human", "z_model"),
                   labels=c("Human", "Model")) +
  ylab("Probability of Two Agents") +
  theme_classic() +
  theme(aspect.ratio=1.0,
        legend.position="none",
        axis.text.x=element_text(hjust=1.0, angle=45))

plot_5 = plot_grid(plot_3, plot_4, nrow=1)
plot_5
```

*Work-in-progress*

## Predicting goal inference as a function of goal and entrance distance

Now we test whether an alternative model that uses simple distance heuristics performs better than our model. This alternative model consists of a multinomial logistic regression that attempts to predict which goal the agent was going for using (1) the distance between the observation and each goal, (2) the distance between the observation and the nearest entrance, and (3) the interactions between (1) and (2).

```{r alternative_model_distance_heuristics}
# Read in the distances between the observation and each goal and entrance.
data_5 = read_csv("data/experiment_3/alternative/distances.csv")

# Stitch the predictors to the mean participant data.
# Stitch the predictors to the mean z-scored participant data.
data_6 = data_4 %>%
  select(map, mean_human, mean_z_human) %>%
  left_join(data_5)

# Build a LASSO regression.
set.seed(0)
predictions = c()
for (m in 1:length(data_6$map)) {
  # Set up an array of random lambda parameters.
  lambda = 10^seq(10, -2, length=100)

  # Full model.
  x = model.matrix(
    mean_z_human ~ A_d1*B_d1*C_d1*A_d2*B_d2*C_d2*doors_d1*doors_d2*num_doors,
    data_6)[,-1]
  y = data_6$mean_z_human
  
  training_trials = setdiff(1:nrow(data_6), m)
  x_train = x[training_trials,]
  y_train = y[training_trials]
  x_test = x[m,]
  y_test = y[m]
  
  # Train the model and find the lambda parameter with the best fit.
  lasso.mod <- glmnet(x_train, y_train, alpha=1, lambda=lambda)
  cv.out <- cv.glmnet(x_train, y_train, alpha=1)
  bestlam <- cv.out$lambda.min
  
  # Generate and store the prediction using the test data.
  lasso.pred = predict(lasso.mod, s=bestlam, newx=t(x_test))
  predictions = c(predictions, as.numeric(lasso.pred))
}
cor(predictions, data_6$mean_z_human)

alternative = data.frame(
  map = data_6$map,
  predictions = predictions
)
```

```{r model_comparison}
# Stitch the alternative model predictions to the mean participant judgments and our model predictions.
data_7 = data_4 %>%
  select(map, mean_z_human, z_model) %>%
  left_join(alternative) %>%
  rename(human=mean_z_human, model=z_model, alternative=predictions)

# data_17 = data_6 %>%
#   # select(map, goal, mean_human, model) %>%
#   select(unique_id, map, num_agents) %>%
#   left_join(., gather(data.frame(map=data_16$map,
#                                  A=fitted(alternative_2)[,1],
#                                  B=fitted(alternative_2)[,2],
#                                  C=fitted(alternative_2)[,3]),
#                       goal, alternative, A, B, C))

# Define the bootstrap function for the bootstrap statistic.
compute_cor = function(data, indices) {
  return(cor(data$alternative[indices], data$human[indices], method="pearson"))
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the bootstrapped 95% CI.
set.seed(seed)
cor_3 = cor(data_7$alternative, data_7$human, method="pearson")
cor_3_bootstrap = compute_bootstrap(data_7)
cor_3_ci = data.frame(
  lower=cor_3_bootstrap[4],
  upper=cor_3_bootstrap[5]
)

# Define the bootstrap function for the bootstrap statistic.
compute_cor_diff = function(data, indices) {
  cor_alternative = cor(data$alternative[indices], data$mean_human[indices], method="pearson")
  cor_model = cor(data$model[indices], data$mean_human[indices], method="pearson")
  return(cor_alternative-cor_model)
}

# Define the bootstrap function to simulate the data.
compute_bootstrap = function(data) {
  # Run the simulations.
  simulations = boot(data=data,
                     statistic=compute_cor_diff,
                     R=10000)
  
  return(boot.ci(simulations, type="bca")$bca)
}

# Compute the correlation difference and the bootstrapped 95% CI.
set.seed(seed)
cor_4 = cor_3 - cor_2
cor_4_bootstrap = compute_bootstrap(data_17)
cor_4_ci = data.frame(
  lower=cor_4_bootstrap[4],
  upper=cor_4_bootstrap[5]
)
```

The Pearson correlation of the alternative model is _r_ = `r round(cor_3, 2)` (95% CI: `r round(cor_3_ci$lower, 2)`-`r round(cor_3_ci$upper, 2)`). The correlation difference between the alternative model and our model is $\Delta$_r_ = `r abs(round(cor_4, 2))` (95% CI: `r abs(round(cor_4_ci$upper, 2))`-`r abs(round(cor_4_ci$lower, 2))`).
